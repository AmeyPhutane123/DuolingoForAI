# How do Transformers work?

Transformers use self-attention mechanisms to understand context in text. This allows them to generate better predictions and handle long-range dependencies in language.
